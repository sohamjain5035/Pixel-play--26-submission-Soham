{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#cell-0 imports,some parameters set,paths\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport csv\nfrom pathlib import Path\nimport cv2 #for frame resizing and optical flow\nfrom skimage.feature import hog #for accesing HOG A\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import    IsolationForest\n\nfrom tqdm import tqdm #for seeing the progress as a percentage \n\n#frame resolution\nFRAME_WIDTH=256\nFRAME_HEIGHT=256\nFRAME_SIZE=(FRAME_WIDTH,FRAME_HEIGHT)\n#PARAMETERS FOR OPTICAL FLOW \nFlow_parameters=dict(orientations=9,pixels_per_cell=(8*8),cells_per_block=(2,2),block_norm='L2-Hys',feature_vector=True)\n\n#TEMPORAL FRAME OFFSETS FOR MOTION MODELLING\nDelta_short=1 #short term motion (t-1)\nDelta_long=5#long term motion(t-5)\n#PATHS\nTRAIN_VIDEOS_DIR = \"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos\"\nTEST_VIDEOS_DIR  =  \"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos\"\nSUBMISSION_PATH=\"submission.csv\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T13:49:22.810629Z","iopub.execute_input":"2026-01-07T13:49:22.810917Z","iopub.status.idle":"2026-01-07T13:49:27.094190Z","shell.execute_reply.started":"2026-01-07T13:49:22.810889Z","shell.execute_reply":"2026-01-07T13:49:27.093060Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# cell-1A: load videos, fix flipped frames, resize, grayscale\n\ndef load_and_preprocess_videos(videos_dir):\n\n    processed_videos = {}\n    flipped_frames = []\n\n    video_folders = sorted([d for d in os.listdir(videos_dir) if d.isdigit()], key=lambda x: int(x))\n    \n    for video_id in tqdm(video_folders):\n\n        video_path = os.path.join(videos_dir, video_id)\n        frame_files = sorted(\n            glob.glob(os.path.join(video_path, \"*.jpg\")),\n            key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n        )\n\n        frames_gray = []\n        frame_ids = []\n\n        for frame_path in frame_files:\n\n            name = os.path.basename(frame_path)\n            num = int(os.path.splitext(name)[0])\n            frame_id = video_id + \"_\" + str(num)\n\n            frame = cv2.imread(frame_path)\n            if frame is None:\n                continue\n\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)#RGB TO GRAYSCALE\n            h, w = gray.shape\n\n            # computING  edge energy using sobel\n            sobel_y = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n\n            top_energy = np.mean(np.abs(sobel_y[:h//3, :]))\n            bottom_energy = np.mean(np.abs(sobel_y[2*h//3:, :]))\n\n            # if top is greater than bottom, frame  flipped\n            if top_energy > bottom_energy:\n                frame = cv2.flip(frame, -1)\n                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n                flipped_frames.append(frame_id)\n\n            # resize to 256*256 (128*128 didnt worked well) after fixing orientation\n            gray = cv2.resize(gray, FRAME_SIZE, interpolation=cv2.INTER_AREA)\n\n            frames_gray.append(gray)\n            frame_ids.append(frame_id)\n\n        processed_videos[video_id] = {\n            \"frames\": frames_gray,\n            \"ids\": frame_ids\n        }\n\n    print(\"Total flipped frames corrected:\", len(flipped_frames))\n    \n\n    return processed_videos\n\n\n# apply preprocessing\n\ntrain_data = load_and_preprocess_videos(TRAIN_VIDEOS_DIR)\ntest_data  = load_and_preprocess_videos(TEST_VIDEOS_DIR)\n\n\nprint('preprocessing done')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-2:  HOG feature extraction\n\ndef extracting_hog_features(video_data):\n\n    hog_features = []\n    frame_ids = []\n\n    for video_id in tqdm(video_data.keys()):\n\n        frames = video_data[video_id][\"frames\"]\n\n        ids    = video_data[video_id][\"ids\"]\n\n        for gray, fid in zip(frames, ids):\n\n            # getting HOG feature  (appearance features)\n            hog_feat = hog(gray, **HOG_PARAMS)\n\n            hog_features.append(hog_feat)\n\n            frame_ids.append(fid)\n\n    hog_features = np.array(hog_features)\n    return hog_features, frame_ids\n# applying on train and test\n\nhog_train_raw, hog_train_ids = extracting_hog_features(train_data)\nhog_test_raw,  hog_test_ids  = extracting_hog_features(test_data)\nprint(\"hog feature matrix made now pca to be applied\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-3:applying  PCA on  HOG features \nhog_pca = PCA( n_components=128,whiten=True,random_state=42) #per frame now only 128 most imp features remain and whiten true so that all 128 compents gets var=1 and random_state=42 so that rerunning does not change results\n\n# fitting  only on training data and not test to prevent data leakage and then transforming train data \nhog_train_pca = hog_pca.fit_transform(hog_train_raw)\n\n#test data getting same redn technique based on train \nhog_test_pca = hog_pca.transform(hog_test_raw)\nprint('dimensionality redn done ')\n\n\n\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-4: extracting optical flow based 21 motion features\n\ndef extracting_optical_flow_features(video_data):\n\n    all_flow_features = []\n    all_frame_ids = []\n\n    \n\n    for video_id in tqdm(video_data.keys()):\n\n        frames = video_data[video_id][\"frames\"]\n        ids    = video_data[video_id][\"ids\"]\n        prev_gray = None\n        prev_mag  = None\n\n        for gray, fid in zip(frames, ids):\n\n            # first frame has no previous reference so filling it 21D as zero\n            if prev_gray is None:\n                all_flow_features.append(np.zeros(21, dtype=np.float32))\n                all_frame_ids.append(fid)\n                prev_gray = gray\n                prev_mag = np.zeros_like(gray, dtype=np.float32)\n                continue\n\n\n            # dense optical flow \n            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, **FLOW_PARAMS)\n\n            vx = flow[..., 0]\n            vy = flow[..., 1]\n            #convering vx and vy to to polar speed(mag) and angle(ang)\n            mag, ang = cv2.cartToPolar(vx, vy)\n\n            # calculating 8 mag related  features for a frame\n            mag_mean = mag.mean()\n            mag_std  = mag.std()\n            mag_p50  = np.percentile(mag, 50)\n            mag_p75  = np.percentile(mag, 75)\n            mag_p90  = np.percentile(mag, 90)\n            mag_p95  = np.percentile(mag, 95)\n            mag_max  = mag.max()\n            mag_sparse = np.mean(mag > mag_p90)\n\n            mag_features = [mag_mean, mag_std, mag_p50, mag_p75, mag_p90, mag_p95, mag_max, mag_sparse]\n                           \n\n            # cal 8 angle related features and 1 entropy for detecting randomness\n            dir_hist, _ = np.histogram(ang, bins=8, range=(0, 2*np.pi), density=True)\n            dir_entropy = -np.sum(dir_hist * np.log(dir_hist + 1e-6))\n\n            # cal coherence feature \n            mean_vx = vx.mean()\n            mean_vy = vy.mean()\n            coherence = np.sqrt(mean_vx**2 + mean_vy**2) / (mag_mean + 1e-8)#1e-8 bcz when i ran first time i got some error bcz mag.mean is zero for all video's first frame\n\n            # 3 accelration related features making\n            acc = mag - prev_mag#change in speed no time term bcz i treat time gap of 1 frama as 1s\n            acc_mean = acc.mean()\n            acc_std  = acc.std()\n            acc_p90  = np.percentile(acc, 90)\n\n            acc_features = [acc_mean, acc_std, acc_p90]\n\n            # all 21 d feature matrix\n            flow_feat = np.concatenate([mag_features, dir_hist.tolist(),   [dir_entropy],[coherence],acc_features ])\n\n            all_flow_features.append(flow_feat)\n            all_frame_ids.append(fid)\n            #updating frames and speed \n            prev_gray = gray\n            prev_mag  = mag\n\n    flow_features = np.array(all_flow_features, dtype=np.float32)\n\n    return flow_features, all_frame_ids\n\n\n# applying it on  on train and test sets \n\nflow_train, flow_train_ids = extracting_optical_flow_features(train_data)\nflow_test,  flow_test_ids  = extracting_optical_flow_features(test_data)\nprint(\"completed all 21 optical featrires\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-5: temporal smoothing and 21to 63 d conversion of optical flow features\n\ndef temporal_smooth_and_deltas(flow_features, video_data):\n\n    enhanced_features = []\n    enhanced_ids = []\n\n    cursor = 0   # pointer over flat flow feature list\n\n    for video_id in tqdm(video_data.keys()):\n\n        frames = video_data[video_id][\"frames\"]\n        ids    = video_data[video_id][\"ids\"]\n\n        num_frames = len(frames)\n\n        # extracting flow features for this video only so that features of one video does not mix with other\n        video_flow = flow_features[cursor: cursor + num_frames]\n        cursor += num_frames\n\n        #applying smoothing 3 window smoothing to avoid uncertain spikes in normal motion bcz some images were kind of blurredor high vingnette so this will help to avoid\n        smoothed = []\n\n        for t in range(num_frames):\n            if t == 0 or t == num_frames - 1:\n                smoothed.append(video_flow[t])\n            else:\n                smoothed.append((video_flow[t-1] + video_flow[t] + video_flow[t+1]) / 3)\n\n        smoothed = np.array(smoothed)\n\n        # using delta logic \n        for t in range(num_frames):\n\n            base = smoothed[t]\n\n            # delta (t-1)\n            if t - DELTA_SHORT >= 0:\n                delta_short = base - smoothed[t - DELTA_SHORT]\n            else:\n                delta_short = np.zeros_like(base)\n\n            # delta (t-5)\n            if t - DELTA_LONG >= 0:\n                delta_long = base - smoothed[t - DELTA_LONG]\n            else:\n                delta_long = np.zeros_like(base)\n\n            # final 63-D feature\n            enhanced = np.concatenate([base, delta_short, delta_long])\n\n            enhanced_features.append(enhanced)\n            enhanced_ids.append(ids[t])\n\n    enhanced_flow_features = np.array(enhanced_features, dtype=np.float32)\n\n    return enhanced_flow_features, enhanced_ids\n\n\n# applying this  on train and test\n\nflow_train_enhanced, flow_train_ids = temporal_smooth_and_deltas(flow_train, train_data)\nflow_test_enhanced,  flow_test_ids  = temporal_smooth_and_deltas(flow_test, test_data)\n\n\nprint(\"Temporal smoothing + deltas completed move to next cell\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-6: mixing 128 hog and 63 flow features \nX_train = np.concatenate([hog_train_pca, flow_train_enhanced], axis=1)\nX_test  = np.concatenate([hog_test_pca,  flow_test_enhanced],  axis=1)\n\n\n# normalizing bcz these 191 features can have very diff scales\nscaler = StandardScaler()\n\n# fit only on training data\nX_train_scaled = scaler.fit_transform(X_train)\n\n# apply same scaling to test\nX_test_scaled = scaler.transform(X_test)\n# storing  final ids\nfinal_train_ids = hog_train_ids\nfinal_test_ids  = hog_test_ids\n\nprint('feature matrix ready for isoln forest')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# cell-7: isolation forest training + per-video anomaly normalization\n\niso_forest = IsolationForest(\n    n_estimators=400,\n    max_samples=\"auto\",\n    contamination=0.10,\n    max_features=1.0,\n    bootstrap=False,\n    n_jobs=-1,\n    random_state=42\n)\n\n# train only on training (mostly normal) data\niso_forest.fit(X_train_scaled)\n\nprint(\" training done\")\n\n\n# raw anomaly scores\n# decision_function: higher = more normal\n# multiply by -1 so higher = more anomalous\n\ntrain_scores_raw = -iso_forest.decision_function(X_train_scaled)\ntest_scores_raw  = -iso_forest.decision_function(X_test_scaled)\n\n\n#  per-video normalization \n\ndef per_video_normalize(scores, video_data, frame_ids):\n\n    norm_scores = np.zeros_like(scores, dtype=np.float32)\n\n    cursor = 0\n\n    for video_id in video_data.keys():\n\n        num_frames = len(video_data[video_id][\"frames\"])\n        video_scores = scores[cursor : cursor + num_frames]\n\n        vmin = video_scores.min()\n        vmax = video_scores.max()\n\n        video_scores_norm = (video_scores - vmin) / (vmax - vmin + 1e-8)\n\n        norm_scores[cursor : cursor + num_frames] = video_scores_norm\n        cursor += num_frames\n\n    return norm_scores\n\n\n# apply per-video normalization\nfinal_train_scores = per_video_normalize(train_scores_raw, train_data, final_train_ids)\nfinal_test_scores  = per_video_normalize(test_scores_raw,  test_data,  final_test_ids)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-8: submission csv generation\n# building rows\n\nsubmission_rows = list(zip(final_test_ids, final_test_scores))\nassert len(submission_rows) == TOTAL_EXPECTED_ROWS\n#writing csv \nwith open(SUBMISSION_PATH, \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"Id\", \"Predicted\"])\n\n    for Id, score in submission_rows:\n        writer.writerow([Id, float(score)])\n\nprint(\"Submission file saved to:\", SUBMISSION_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#boosting the natural csv\nimport pandas as pd\nimport numpy as np\n\nINPUT_CSV  = \"submission.csv\"\nOUTPUT_CSV = \"boosted_submission.csv\"\n\nTOP_PCT = 0.01     \nBOOST   = 0.15     \n\ndf = pd.read_csv(INPUT_CSV)\nscores = df[\"Predicted\"].values.copy()\n\nN = len(scores)\nk = max(1, int(TOP_PCT * N))\n\ntop_idx = np.argsort(scores)[-k:]\nscores[top_idx] = scores[top_idx] + BOOST\n\n\nscores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n\n\nout = df.copy()\nout[\"Predicted\"] = scores\nout.to_csv(OUTPUT_CSV, index=False)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}