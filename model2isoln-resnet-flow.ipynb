{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#cell 0 imports and paths \nimport os\nimport glob\nimport csv\nfrom pathlib import Path\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm\n\n# frame size\nFRAME_WIDTH  = 256\nFRAME_HEIGHT = 256\nFRAME_SIZE   = (FRAME_WIDTH, FRAME_HEIGHT)\n\n#paths\n\nTRAIN_VIDEOS_DIR =\"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos\"\nTEST_VIDEOS_DIR  = \"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos\"\n\nSUBMISSION_PATH =  \"submission.csv\"\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 1 datapreprocessing\n\nimport re\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nimport sys\n\n\ndef extract_frame_number(filename):\n    nums = re.findall(r\"\\d+\", filename)\n    if not nums:\n        raise ValueError(f\"No numeric frame index found in {filename}\")\n    return int(nums[-1])\n\n\n#flipping logic\ndef is_upside_down_sobel(gray):\n    h = gray.shape[0]\n\n    sobel_y = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n\n    top_energy    = np.mean(np.abs(sobel_y[:h // 3, :]))\n    bottom_energy = np.mean(np.abs(sobel_y[2 * h // 3:, :]))\n\n    return top_energy > bottom_energy\n\n\n#convering images to Imagenet normalised tensors\nresnet_transform = T.Compose([\n    T.ToTensor(),\n    T.Normalize(\n        mean=[0.485, 0.456, 0.406],   # ImageNet mean\n        std=[0.229, 0.224, 0.225]     # ImageNet std\n    )\n])\n\ndef load_and_preprocess_videos(videos_dir, dataset_name=\"train\", max_visualize=3):\n\n    processed_videos = {}\n    rotated_records = []\n    visualized = 0\n\n    video_folders = sorted(\n        [d for d in os.listdir(videos_dir) if d.isdigit()],\n        key=lambda x: int(x)\n    )\n\n    for video_id in tqdm(video_folders):\n\n        video_path = videos_dir / video_id\n\n        frame_files = sorted(\n            glob.glob(str(video_path / \"*.jpg\")),\n            key=lambda x: extract_frame_number(os.path.basename(x))\n        )\n\n        frames_gray = []\n        frames_rgb  = []\n        frame_ids   = []\n\n        for frame_path in frame_files:\n\n            fname = os.path.basename(frame_path)\n            frame_number = extract_frame_number(fname)\n            frame_id = f\"{video_id}_{frame_number}\"\n\n            img_bgr = cv2.imread(frame_path)\n            if img_bgr is None:\n                print(\"Warning: could not read\", frame_path)\n                continue\n\n            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n            gray    = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n\n            # upside-down fix ( SOBEL LOGIC)\n            flipped = False\n            if is_upside_down_sobel(gray):\n                img_rgb = cv2.flip(img_rgb, -1)\n                gray    = cv2.flip(gray, -1)\n                flipped = True\n                rotated_records.append((video_id, frame_number))\n\n            # resize AFTER all flips corrected\n            img_rgb = cv2.resize(img_rgb, FRAME_SIZE, interpolation=cv2.INTER_AREA)\n            gray    = cv2.resize(gray, FRAME_SIZE, interpolation=cv2.INTER_AREA)\n\n            # store grayscale (classical branch)\n            frames_gray.append(gray)\n\n            # store RGB (resnET branch)\n            frames_rgb.append(resnet_transform(img_rgb))\n\n            frame_ids.append(frame_id)\n\n           \n\n        processed_videos[video_id] = {\n            \"gray\": frames_gray,\n            \"rgb\":  frames_rgb,\n            \"ids\":  frame_ids\n        }\n\n  \n    print(\"Total corrected frames:\", len(rotated_records))\n    return processed_videos\n\n\n\ntrain_data = load_and_preprocess_videos( TRAIN_VIDEOS_DIR, dataset_name=\"train\")\n  \ntest_data = load_and_preprocess_videos(TEST_VIDEOS_DIR, dataset_name=\"test\")\n\n   \n    \n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 2 optical flow features(21d)\n\n# optical flow parameters \nFLOW_PARAMS = dict(\n    pyr_scale=0.5,\n    levels=3,\n    winsize=15,\n    iterations=3,\n    poly_n=5,\n    poly_sigma=1.2,\n    flags=0\n)\n\ndef extracting_optical_flow_features(video_data):\n\n    all_flow_features = []\n    all_frame_ids = []\n\n    for video_id in tqdm(video_data.keys()):\n\n        gray_frames = video_data[video_id][\"gray\"]\n        ids         = video_data[video_id][\"ids\"]\n\n        prev_gray = None\n        prev_mag  = None\n\n        for gray, fid in zip(gray_frames, ids):\n\n            # first frame has no previous reference\n            if prev_gray is None:\n                all_flow_features.append(np.zeros(21, dtype=np.float32))\n                all_frame_ids.append(fid)\n                prev_gray = gray\n                prev_mag  = np.zeros_like(gray, dtype=np.float32)\n                continue\n\n            # dense optical flow\n            flow = cv2.calcOpticalFlowFarneback(\n                prev_gray, gray, None, **FLOW_PARAMS\n            )\n\n            vx = flow[..., 0]\n            vy = flow[..., 1]\n            mag, ang = cv2.cartToPolar(vx, vy)\n\n            # magnitude features (8)\n            mag_mean = mag.mean()\n            mag_std  = mag.std()\n            mag_p50  = np.percentile(mag, 50)\n            mag_p75  = np.percentile(mag, 75)\n            mag_p90  = np.percentile(mag, 90)\n            mag_p95  = np.percentile(mag, 95)\n            mag_max  = mag.max()\n            mag_sparse = np.mean(mag > (mag_p90 + 1e-6))\n\n            mag_features = [mag_mean, mag_std, mag_p50, mag_p75,\n                            mag_p90, mag_p95, mag_max, mag_sparse]\n\n            # direction features (8 + 1)\n            dir_hist, _ = np.histogram(\n                ang, bins=8, range=(0, 2*np.pi), density=True\n            )\n\n            dir_entropy = -np.sum(dir_hist * np.log(dir_hist + 1e-6))\n\n            # coherence (1) \n            mean_vx = vx.mean()\n            mean_vy = vy.mean()\n            coherence = np.sqrt(mean_vx**2 + mean_vy**2) / (mag_mean + 1e-6)\n\n            #acceleration features(3)\n            acc_mean = acc.mean()\n            acc_std  = acc.std()\n            acc_p90  = np.percentile(acc, 90)\n\n            acc_features = [acc_mean, acc_std, acc_p90]\n\n            #  ALL 21 FLOW FEATURES\n            flow_feat = np.concatenate([\n                mag_features,\n                dir_hist.tolist(),\n                [dir_entropy],\n                [coherence],\n                acc_features\n            ])\n\n            all_flow_features.append(flow_feat.astype(np.float32))\n            all_frame_ids.append(fid)\n             #UPDATING FRAMES\n            prev_gray = gray\n            prev_mag  = mag\n\n    flow_features = np.array(all_flow_features, dtype=np.float32)\n\n    return flow_features, all_frame_ids\n\n\n# apply on train and test\n\nflow_train, flow_train_ids = extracting_optical_flow_features(train_data)\nflow_test,  flow_test_ids  = extracting_optical_flow_features(test_data)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell3 Isolation forest training\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\n\n# normalising the 21 flow features which were of diff scales\nflow_scaler = StandardScaler()\n\nX_train_flow = flow_scaler.fit_transform(X_train_flow)\nX_test_flow  = flow_scaler.transform(X_test_flow)\n#training isoln forest\niso_forest = IsolationForest(**IF_PARAMS)\niso_forest.fit(X_train_flow)\n\n# raw anomaly scores\n# decision_function: higher = more normal\n# multiply by -1 so higher = more anomalous\n\nif_train_scores = -iso_forest.decision_function(X_train_flow)\nif_test_scores  = -iso_forest.decision_function(X_test_flow)\n\n#storing for the final fusion\nfinal_if_train_scores = if_train_scores\nfinal_if_test_scores  = if_test_scores\n\nfinal_train_ids = flow_train_ids\nfinal_test_ids  = flow_test_ids\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell4 2nd RESNET BRANCH \n\nimport torchvision.models as models\nimport torch.nn as nn\n\n#loading pretrained resnet 18\nresnet = models.resnet18(pretrained=True)\n\n# removing the default classifier layer of resnet\nresnet.fc = nn.Identity()\n\nresnet = resnet.to(DEVICE)\nresnet.eval()\n\nRESNET_FEATURE_DIM = 512\nRESNET_WINDOW = 8\n\ndef temporal_pool_features(features, window):\n\n    T, D = features.shape\n    pooled = np.zeros_like(features)\n\n    for t in range(T):\n        start = max(0, t - window)\n        end   = min(T, t + window + 1)\n\n        pooled[t] = features[start:end].max(axis=0)\n\n    return pooled\n#extraction of 512 resnet features \ndef extract_resnet_features(video_data):\n\n    all_features = []\n    all_ids = []\n\n    with torch.no_grad():\n        for video_id in tqdm(video_data.keys()):\n\n            rgb_frames = video_data[video_id][\"rgb\"]\n            ids        = video_data[video_id][\"ids\"]\n\n            BATCH_SIZE = 64\n            num_frames = len(rgb_frames)\n            video_feats = []\n\n            for i in range(0, num_frames, BATCH_SIZE):\n\n                batch = torch.stack(rgb_frames[i:i+BATCH_SIZE]).to(DEVICE)\n                feats = resnet(batch)              # [B, 512]\n                video_feats.append(feats.cpu().numpy())\n\n            feats_raw = np.vstack(video_feats)\n\n            # apply temporal max pooling\n            feats_pooled = temporal_pool_features(feats_raw, RESNET_WINDOW)\n\n            all_features.append(feats_pooled)\n            all_ids.extend(ids)\n\n    features = np.vstack(all_features)\n\n    return features, all_ids\n\n\n#applying on training and testing\nresnet_train_feats, resnet_train_ids = extract_resnet_features(train_data)\nresnet_test_feats,  resnet_test_ids  = extract_resnet_features(test_data)\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 5 computing anamoly scores based on 512 resnet features\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\nkmeans.fit(resnet_train_feats)\n\n#computing distance to the nearest cluster\ndef compute_distance_to_nearest_cluster(features, kmeans_model):\n\n    # calculating distances to all clusters [N, k]\n    all_dists = kmeans_model.transform(features)\n\n    # choosing closest cluster(normal behavior)\n    min_dists = all_dists.min(axis=1)\n    return min_dists\n\ndeep_train_scores = compute_distance_to_nearest_cluster(resnet_train_feats, kmeans)\ndeep_test_scores  = compute_distance_to_nearest_cluster(resnet_test_feats,  kmeans)\n\n#  storing for fusion\nfinal_deep_train_scores = deep_train_scores\nfinal_deep_test_scores  = deep_test_scores\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 6 fusing the both branch and calcluating anamoly scores\n\n\n#sigmoid of flow features\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\nif_gate_test = sigmoid(final_if_test_scores)\n\n# anomaly = semantic novelty Ã— motion unreliability\nraw_fused_scores = final_deep_test_scores * if_gate_test\n\n\n#normalising gloablaly \nscore_min = raw_fused_scores.min()\nscore_max = raw_fused_scores.max()\n\nfinal_test_scores = (raw_fused_scores - score_min) / (score_max - score_min + 1e-8)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 7 csv genration\n\nTOTAL_EXPECTED_ROWS = 11706\nCSV_PATH = SUBMISSION_PATH\n\n#building rows\nsubmission_rows = []\n\nfor fid, score in zip(final_test_ids, final_test_scores):\n\n    vid_str, frame_str = fid.split(\"_\")\n\n    vid_num   = int(vid_str)\n    frame_num = int(frame_str)\n\n    fixed_id = f\"{vid_num}_{frame_num}\"\n    submission_rows.append((fixed_id, float(score)))\n\n\n#numerical sorting\n\nsubmission_rows.sort(key=lambda x: (int(x[0].split(\"_\")[0]), int(x[0].split(\"_\")[1])))\n\n\n#writing csv\nwith open(CSV_PATH, \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"Id\", \"Predicted\"])\n    writer.writerows(submission_rows)\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nINPUT_CSV  = \"submission.csv\"\nOUTPUT_CSV = \"boosted_submission.csv\"\n\nTOP_PCT = 0.01     \nBOOST   = 0.15     \n\ndf = pd.read_csv(INPUT_CSV)\nscores = df[\"Predicted\"].values.copy()\n\nN = len(scores)\nk = max(1, int(TOP_PCT * N))\n\ntop_idx = np.argsort(scores)[-k:]\nscores[top_idx] = scores[top_idx] + BOOST\n\n\nscores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n\n\nout = df.copy()\nout[\"Predicted\"] = scores\nout.to_csv(OUTPUT_CSV, index=False)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}